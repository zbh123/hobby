卷积神经网络分为：
输入层读入经过规则化（统一大小）的图像，每一层的每个神经元将前一层的一组小的局部近邻的单元作为输入，也就是局部感受野和权值共享，
神经元抽取一些基本的视觉特征，比如边缘、角点等，这些特征之后会被更高层的神经元所使用。卷积神经网络通过卷积操作获得特征图，
每个位置，来自不同特征图的单元得到各自不同类型的特征。一个卷积层中通常包含多个具有不同权值向量的特征图，使得能够保留图像更丰富的特征。
卷积层后边会连接池化层进行降采样操作，一方面可以降低图像的分辨率，减少参数量，另一方面可以获得平移和形变的鲁棒性。卷积层和池化层的交替分布，
使得特征图的数目逐步增多，而且分辨率逐渐降低，是一个双金字塔结构

1、卷积层：
所需参数：shape：1）卷积核大小（size，size）2）输入通道：移动步长，3）输出通道：填充（输出轴的大小）
例子：tf.random_normal(shape, stddev=0.01)，shape=[3, 3, 3, 32]
原理：根据卷积核，对填充之后的数据，进行卷积操作，一般是将数据分为RGB三层，对这三层在相应位置与卷积核进行乘积（对应位置相乘，然后相加），得到输出
2、激励层（激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)）：
所需参数：输入：卷积后的输出，权重因子（偏置值）类似于x + b
例子：tf.nn.relu(conv2d(x, W1) + b1)， b1 = tf.random_normal([32])
原理：权重因子的大小必须与输出的行相同，相当于权重因子，用于调节函数
3、池化层
所需参数：输入：1）激励之后的输出， 2）ksize：池化窗的大小，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化
3）strides：窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
实例：tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
原理：在池化窗内选取最大值（max_pool），作为输出值，用于凸显特征值。主要作用：降维

================================
全连接层之前是提取特征，全连接的作用是分类，将特征进行分类，以便区别不同事物

4、全连接层：连接所有的特征，将输出值送给分类器
原理：将多次卷积之后的结果，按照一定规则送给分类器，用于识别图片中的某些事物的特征。
