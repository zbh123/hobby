卷积神经网络分为：
输入层读入经过规则化（统一大小）的图像，每一层的每个神经元将前一层的一组小的局部近邻的单元作为输入，也就是局部感受野和权值共享，
神经元抽取一些基本的视觉特征，比如边缘、角点等，这些特征之后会被更高层的神经元所使用。卷积神经网络通过卷积操作获得特征图，
每个位置，来自不同特征图的单元得到各自不同类型的特征。一个卷积层中通常包含多个具有不同权值向量的特征图，使得能够保留图像更丰富的特征。
卷积层后边会连接池化层进行降采样操作，一方面可以降低图像的分辨率，减少参数量，另一方面可以获得平移和形变的鲁棒性。卷积层和池化层的交替分布，
使得特征图的数目逐步增多，而且分辨率逐渐降低，是一个双金字塔结构

1、卷积层：
所需参数：shape：1）卷积核大小（size，size）2）输入通道：移动步长，3）输出通道：填充（输出轴的大小）。
输出：大小与输入值相同，卷积不会改变输入值的大小
例子：tf.random_normal(shape, stddev=0.01)，shape=[3, 3, 3, 32]
原理：根据卷积核，对填充之后的数据，进行卷积操作，一般是将数据分为RGB三层，对这三层在相应位置与卷积核进行乘积（对应位置相乘，然后相加），得到输出
2、激励层（激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)）：
所需参数：输入：卷积后的输出，权重因子（偏置值）类似于x + b
输出：将输入值按照激励函数标准化（-1~1或0~1）
例子：tf.nn.relu(conv2d(x, W1) + b1)， b1 = tf.random_normal([32])
原理：权重因子的大小必须与输出的行相同，相当于权重因子，用于调节函数
3、池化层
所需参数：输入：1）激励之后的输出， 2）ksize：池化窗的大小，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化
3）strides：窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
输出：池化会改变输入值的大小，若输入值大小为：36*64，经历池化[1, 2, 2, 1]之后变为18*32
实例：tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
原理：在池化窗内选取最大值（max_pool），作为输出值，用于凸显特征值。主要作用：降维

================================
假设全连接层前面连接的是一个卷积层，这个卷积层的输出是64个特征，每个特征的大小是7X7，那么在将这些特征输入给全连接层之前会将这些特
征通过tf.reshape转化为成N1的向量（这个时候N就是64X7X7=3136）


全连接层之前是提取特征，全连接的作用是分类，将特征进行分类，以便区别不同事物

4、全连接层：连接所有的特征，将输出值送给分类器,即在训练的时候将训练对象的特征保存起来，
输入参数：权重大小（shape=[4 * 10 * 64, 1024]），shape大小是根据一层层卷积池化计算得来的，若原参数大小[64*36]经过三次卷积、池化分别变为[32*18]、[16*9]、[8*5]（可根据需要选择进一或退一）
全连接层不做卷积直接叉乘，weights（mxn）的m可由上面计算得来，后面的n用于调节输出值的大小（可自己定义），用于控制输出值的大小。


全连接之后的常用模块tf.nn.softmax(logits, axis)，logist输入数据，axis延哪个轴进行分辨，输出各特征的概率即预测值，类似于取标准值
在之后run的过程中tf.nn.softmax的返回值作为输入，再加上要预测的对象作为参数，可以返回最终的预测概率
应用：通过Softmax回归，将logistic的预测二分类的概率的问题推广到了n分类的概率的问题
原理：将多次卷积之后的结果，按照一定规则送给分类器，用于识别图片中的某些事物的特征。
